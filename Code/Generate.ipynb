{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paths for data input and weights output\n",
    "data_dir = \"../Data/\"\n",
    "data_file = \"ONeill_trimmed_quarter.txt\"\n",
    "save_weights_dir = '../Trained_Weights/Weights_ONeill_quart_64b_128s_final/'\n",
    "log_dir = \"../Data/log.csv\"\n",
    "charToIndex_json = \"char_to_index.json\"\n",
    "\n",
    "# Parameters\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(unique_chars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # input shape is now (1,1) since we're only feeding the starting character - i.e. the starting note\n",
    "    # for the generated sequence\n",
    "    model.add(Embedding(input_dim = unique_chars, output_dim = 8, batch_input_shape = (1, 1))) \n",
    "  \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(256, stateful = True)) \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add((Dense(unique_chars)))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_sequence(generated_seq):\n",
    "    print(\"\\nRAW generated sequence: \\n\" + generated_seq)\n",
    "    \n",
    "    # the generated sequence ususally contains a few meaningless characters before a newline. Generally the rythm\n",
    "    # and more cohesive notes are generated from the next line onward, so we remove the unnecessary characters.\n",
    "    count = 0\n",
    "    for char in generated_seq:\n",
    "        count += 1\n",
    "        if char == \"\\n\" and generated_seq[count] == \"\\n\":\n",
    "            break\n",
    "    seq_trimmed_before = generated_seq[count+1:]\n",
    "    \n",
    "    # The training data contains multiple songs, each separated by three newline characters. The model has learned this pattern and \n",
    "    # also adds three new line characters by itself. We would like to consider one song at time, so we ignore what follows after\n",
    "    # these ending newlines.\n",
    "    count = 0\n",
    "    for char in seq_trimmed_before:\n",
    "        count += 1\n",
    "        if char == \"\\n\" and seq_trimmed_before[count] == \"\\n\":\n",
    "            break\n",
    "    seq_trimmed = seq_trimmed_before[:count]\n",
    "    \n",
    "    return seq_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(epoch, initial_note, seq_length):\n",
    "    file = open(os.path.join(data_dir, data_file), mode = 'r')\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    # Load character mapping\n",
    "    char_to_index = {char: x for (x, char) in enumerate(sorted(list(set(data))))}\n",
    "    print(\"Unique characters in the training data = {}\".format(len(char_to_index)))  \n",
    "    #with open(os.path.join(data_dir, charToIndex_json)) as f:\n",
    "    #    char_to_index = json.load(f)\n",
    "    index_to_char = {x:char for char, x in char_to_index.items()}\n",
    "    unique_chars = len(index_to_char)\n",
    "    \n",
    "    model = make_model(unique_chars)\n",
    "    model.load_weights(save_weights_dir + \"Weights_{}.h5\".format(epoch))\n",
    "    #model.summary()\n",
    "    sequence_index = [initial_note]\n",
    "    \n",
    "    for _ in range(seq_length):\n",
    "        batch = np.zeros((1, 1))\n",
    "        batch[0, 0] = sequence_index[-1]\n",
    "        \n",
    "        # predict the probabilities for the next input character\n",
    "        predicted_probs = model.predict_on_batch(batch).ravel()\n",
    "        # randomly sample the character based on the probabilities from the network\n",
    "        char_sample = np.random.choice(range(unique_chars), size = 1, p = predicted_probs)\n",
    "        sequence_index.append(char_sample[0])\n",
    "    \n",
    "    # obtain a string of the notes corresponding to each generated character\n",
    "    gen_sequence = ''.join(index_to_char[c] for c in sequence_index)\n",
    "    trimmed = trim_sequence(gen_sequence)\n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters in the training data = 92\n",
      "\n",
      "RAW generated sequence: \n",
      "L#9E/2) (EA,).|\n",
      "D2 A/2A/2 BA | F2 D(E/F/) GG | F2 D G3/2B/2 |\n",
      "({d}A>G) (GA).A | {d}(Bcc) c(2-A/2B/2) | ccA G<A | G3 ||\n",
      "(\n",
      "c/2c/2) | efg ga | eef fdc | B((db/2f/2) gf | ({gf}cc).B | c3 ||\n",
      "\n",
      "\n",
      "X: 24\n",
      "T: The Black Slender Boy\n",
      "M: 3/4\n",
      "L: 1/8\n",
      "B: \"O'Neill's 28\"\n",
      "N: \"Moderate\"\n",
      "N: \"Collected by J.O'Neill\"\n",
      "N:\"Mollected by F. O'Neill\"\n",
      "Z: 1997 by John Chambers <jc@trillian.mit.edu>\n",
      "M: 3/4\n",
      "L: 1/8\n",
      "K:A\n",
      "(d>c) \\\n",
      "| B2 (GG) (E>D) | (D>F) (G2 GG) | {A}(EG) .G.A .B.A A | (Bd) e2 (dd) |\n",
      "| (e{cd}d>).A (GG) | (GG) (.G2 .G) | (Gd) (Be) | (de) d2 Bc | A4 (DA) |\n",
      "| (Bc) (ee) (dc) | (AG) (EG) | (eg) e2 | (ff) (dc) | dB A,z \\\n",
      "| D2 (GA) | (A>A) (EE) | (EA) (Bd) | (de) e2 | (.d c)A F ||\n",
      "\n",
      "\n",
      "X: 248\n",
      "T: the HRameer Wose If mhe Ero\n",
      "B: O'Neill's 299\n",
      "N: \"Moderate\"\n",
      "N: \"Collected by J.O'Neill\"\n",
      "Z: 1997 by John Chambers <jc@trillian.mit.e\n",
      "\n",
      "TRIMMED generated sequence: \n",
      "\n",
      "\n",
      "X: 24\n",
      "T: The Black Slender Boy\n",
      "M: 3/4\n",
      "L: 1/8\n",
      "B: \"O'Neill's 28\"\n",
      "N: \"Moderate\"\n",
      "N: \"Collected by J.O'Neill\"\n",
      "N:\"Mollected by F. O'Neill\"\n",
      "Z: 1997 by John Chambers <jc@trillian.mit.edu>\n",
      "M: 3/4\n",
      "L: 1/8\n",
      "K:A\n",
      "(d>c) \\\n",
      "| B2 (GG) (E>D) | (D>F) (G2 GG) | {A}(EG) .G.A .B.A A | (Bd) e2 (dd) |\n",
      "| (e{cd}d>).A (GG) | (GG) (.G2 .G) | (Gd) (Be) | (de) d2 Bc | A4 (DA) |\n",
      "| (Bc) (ee) (dc) | (AG) (EG) | (eg) e2 | (ff) (dc) | dB A,z \\\n",
      "| D2 (GA) | (A>A) (EE) | (EA) (Bd) | (de) e2 | (.d c)A F ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 80\n",
    "initial_note = 42 # any integer between 0 and the number of uniqe characters in the data set\n",
    "seq_length = 800 # shouldn't be below 300 in order to generate a valid sequence\n",
    "\n",
    "music = generate_sequence(epoch, initial_note, seq_length)\n",
    "\n",
    "print(\"\\nTRIMMED generated sequence: \\n\")\n",
    "\n",
    "print(music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
