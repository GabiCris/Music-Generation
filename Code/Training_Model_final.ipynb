{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paths for data input and weights output\n",
    "data_dir = \"../Data/\"\n",
    "data_file = \"ONeill_trimmed_quarter.txt\"\n",
    "save_weights_dir = '../Trained_Weights/Weights_ONeill_quart_64b_128s_final/'\n",
    "log_dir = \"../Data/log_Oneill_quart_final.csv\"\n",
    "charToIndex_json = \"char_to_index.json\"\n",
    "\n",
    "transfer_weights_path = \"../Trained_Weights/Weights_ONeill_quart_64b_128s/Weights_80.h5\"\n",
    "# Parameters\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to create the batches\n",
    "def get_batches(chars, unique_chars):\n",
    "    char_no = chars.shape[0] # number of characters in the data\n",
    "    batch_chars = int(char_no / BATCH_SIZE)\n",
    "    \n",
    "    # outer loop iterates every time a new batch is created\n",
    "    for start in range(0, batch_chars - SEQ_LENGTH, SEQ_LENGTH):\n",
    "        # number of batches wil be char_no/(BATCH_SIZE * SEQ_LENGTH)\n",
    "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))  \n",
    "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))\n",
    "        # iterates over rows in a batch\n",
    "        for batch_row in range(0, BATCH_SIZE):             \n",
    "            # iterates over columns in a batch\n",
    "            for i in range(0, SEQ_LENGTH): \n",
    "                X[batch_row, i] = chars[batch_row * batch_chars + start + i]\n",
    "                Y[batch_row, i, chars[batch_row * batch_chars + start + i + 1]] = 1 \n",
    "                # by 1 we mark that the next character in the sequence is the correct one\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added 2 more LSTM layers\n",
    "# loading previously computed weights - transfer learning\n",
    "def build_model(batch_size, seq_length, unique_chars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # inputs have to be the same length which is achieved when creating batches\n",
    "    # input dimension will be the number of unique characters in the training data\n",
    "    # output-dimention needs more validation - 8?\n",
    "    model.add(Embedding(input_dim = unique_chars, output_dim = 8, batch_input_shape = (batch_size, seq_length), name = \"embd_1\")) \n",
    "    \n",
    "    # setting return_sequences to True in order to stack multiple LSTM layers\n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True, name = \"lstm_first\"))\n",
    "    model.add(Dropout(0.2, name = \"drp_1\"))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #fully connected layer to connect the output of the lstm layers to the output\n",
    "    model.add(TimeDistributed(Dense(unique_chars)))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    model.load_weights(transfer_weights_path, by_name = True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, epochs = 80):\n",
    "    \n",
    "    # Mapping all unique characters to an index\n",
    "    char_to_index = {char: x for (x, char) in enumerate(sorted(list(set(data))))}\n",
    "    print(\"Unique characters in the training data = {}\".format(len(char_to_index)))  \n",
    "    # Saved the mapping in a json file\n",
    "    with open(os.path.join(data_dir, charToIndex_json), mode = \"w\") as f:\n",
    "        json.dump(char_to_index, f)\n",
    "        \n",
    "    index_to_char = {x: char for (char, x) in char_to_index.items()}\n",
    "    unique_chars = len(char_to_index)\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_model(BATCH_SIZE, SEQ_LENGTH, unique_chars)\n",
    "    model.summary()\n",
    "    # multi-class classification problem - using Categorical Cross entropy as loss function\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    characters = np.asarray([char_to_index[c] for c in data], dtype = np.int32)\n",
    "    print(\"Total number of characters = \"+str(characters.shape[0])) #155222\n",
    "    \n",
    "    epoch_number, loss, accuracy = [], [], []\n",
    "    \n",
    "    # saving training data for furture logging\n",
    "    saved_epoch, loss, accuracy = [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "        last_epoch_loss, last_epoch_accuracy = 0, 0\n",
    "        saved_epoch.append(epoch+1)\n",
    "        \n",
    "        # reading the batches one by one and training the model on each one\n",
    "        for i, (x, y) in enumerate(get_batches(characters, unique_chars)):\n",
    "            last_epoch_loss, last_epoch_accuracy = model.train_on_batch(x, y) \n",
    "            print(\"Batch No.: {}, Loss: {}, Accuracy: {}\".format(i+1, last_epoch_loss, last_epoch_accuracy))\n",
    "        loss.append(last_epoch_loss)\n",
    "        accuracy.append(last_epoch_accuracy)\n",
    "        \n",
    "        # Saving the computed weights each 10th epoch\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            if not os.path.exists(save_weights_dir):\n",
    "                os.makedirs(save_weights_dir)\n",
    "            model.save_weights(os.path.join(save_weights_dir, \"Weights_{}.h5\".format(epoch+1)))\n",
    "            print('Saved weights computed at epoch {} to Weights_{}.h5'.format(epoch+1, epoch+1))\n",
    "    \n",
    "    # Logging the training data into a DataFrame structure to be saved to file after each training\n",
    "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "    log_frame[\"Epoch\"] = saved_epoch\n",
    "    log_frame[\"Loss\"] = loss\n",
    "    log_frame[\"Accuracy\"] = accuracy\n",
    "    log_frame.to_csv(log_dir, index = False)\n",
    "    \n",
    "    # Accuracy Plot\n",
    "    pyplot.plot(accuracy, epoch_number)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters in the training data = 92\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embd_1 (Embedding)           (64, 128, 8)              736       \n",
      "_________________________________________________________________\n",
      "lstm_first (LSTM)            (64, 128, 256)            271360    \n",
      "_________________________________________________________________\n",
      "drp_1 (Dropout)              (64, 128, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (64, 128, 256)            525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (64, 128, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (64, 128, 256)            525312    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (64, 128, 256)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (64, 128, 92)             23644     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (64, 128, 92)             0         \n",
      "=================================================================\n",
      "Total params: 1,346,364\n",
      "Trainable params: 1,346,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Total number of characters = 930105\n",
      "Epoch 1/80\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Batch No.: 1, Loss: 4.5216875076293945, Accuracy: 0.0135498046875\n",
      "Batch No.: 2, Loss: 4.516626358032227, Accuracy: 0.1624755859375\n",
      "Batch No.: 3, Loss: 4.505924701690674, Accuracy: 0.162109375\n",
      "Batch No.: 5, Loss: 4.4064836502075195, Accuracy: 0.15673828125\n",
      "Batch No.: 6, Loss: 4.143061637878418, Accuracy: 0.1561279296875\n",
      "Batch No.: 7, Loss: 3.9954469203948975, Accuracy: 0.1751708984375\n",
      "Batch No.: 8, Loss: 3.946671485900879, Accuracy: 0.156982421875\n",
      "Batch No.: 9, Loss: 3.875278949737549, Accuracy: 0.148193359375\n",
      "Batch No.: 10, Loss: 3.7513949871063232, Accuracy: 0.128173828125\n",
      "Batch No.: 11, Loss: 3.8334174156188965, Accuracy: 0.093017578125\n",
      "Batch No.: 12, Loss: 3.8358216285705566, Accuracy: 0.090087890625\n",
      "Batch No.: 13, Loss: 3.7072696685791016, Accuracy: 0.1239013671875\n",
      "Batch No.: 14, Loss: 3.6910691261291504, Accuracy: 0.1474609375\n",
      "Batch No.: 15, Loss: 3.7159183025360107, Accuracy: 0.1605224609375\n",
      "Batch No.: 16, Loss: 3.6659207344055176, Accuracy: 0.169189453125\n",
      "Batch No.: 17, Loss: 3.632235527038574, Accuracy: 0.1798095703125\n",
      "Batch No.: 18, Loss: 3.7249104976654053, Accuracy: 0.167724609375\n",
      "Batch No.: 19, Loss: 3.72355318069458, Accuracy: 0.162109375\n",
      "Batch No.: 20, Loss: 3.6647181510925293, Accuracy: 0.1727294921875\n",
      "Batch No.: 21, Loss: 3.7375640869140625, Accuracy: 0.162109375\n",
      "Batch No.: 22, Loss: 3.6543161869049072, Accuracy: 0.1669921875\n",
      "Batch No.: 23, Loss: 3.7654306888580322, Accuracy: 0.1519775390625\n",
      "Batch No.: 25, Loss: 3.6204779148101807, Accuracy: 0.1737060546875\n",
      "Batch No.: 26, Loss: 3.743230104446411, Accuracy: 0.1556396484375\n",
      "Batch No.: 27, Loss: 3.7288293838500977, Accuracy: 0.14990234375\n",
      "Batch No.: 28, Loss: 3.647965669631958, Accuracy: 0.1585693359375\n",
      "Batch No.: 29, Loss: 3.6948745250701904, Accuracy: 0.1605224609375\n",
      "Batch No.: 30, Loss: 3.7320570945739746, Accuracy: 0.15283203125\n",
      "Batch No.: 31, Loss: 3.6462879180908203, Accuracy: 0.15478515625\n",
      "Batch No.: 32, Loss: 3.6254992485046387, Accuracy: 0.1683349609375\n",
      "Batch No.: 33, Loss: 3.677121877670288, Accuracy: 0.1661376953125\n",
      "Batch No.: 34, Loss: 3.713855743408203, Accuracy: 0.146728515625\n",
      "Batch No.: 36, Loss: 3.6605448722839355, Accuracy: 0.1685791015625\n",
      "Batch No.: 37, Loss: 3.6365199089050293, Accuracy: 0.16162109375\n",
      "Batch No.: 38, Loss: 3.7247509956359863, Accuracy: 0.1573486328125\n",
      "Batch No.: 39, Loss: 3.742000102996826, Accuracy: 0.1566162109375\n",
      "Batch No.: 40, Loss: 3.572812080383301, Accuracy: 0.173828125\n",
      "Batch No.: 41, Loss: 3.705111026763916, Accuracy: 0.1585693359375\n",
      "Batch No.: 42, Loss: 3.6573684215545654, Accuracy: 0.1607666015625\n",
      "Batch No.: 43, Loss: 3.5916783809661865, Accuracy: 0.1728515625\n",
      "Batch No.: 44, Loss: 3.6146035194396973, Accuracy: 0.1785888671875\n",
      "Batch No.: 45, Loss: 3.683744430541992, Accuracy: 0.16064453125\n",
      "Batch No.: 46, Loss: 3.6023964881896973, Accuracy: 0.168212890625\n",
      "Batch No.: 47, Loss: 3.7115631103515625, Accuracy: 0.1563720703125\n",
      "Batch No.: 48, Loss: 3.6673572063446045, Accuracy: 0.157470703125\n",
      "Batch No.: 49, Loss: 3.6842055320739746, Accuracy: 0.161376953125\n",
      "Batch No.: 50, Loss: 3.658602714538574, Accuracy: 0.164306640625\n",
      "Batch No.: 51, Loss: 3.6564249992370605, Accuracy: 0.164794921875\n",
      "Batch No.: 52, Loss: 3.674119472503662, Accuracy: 0.156494140625\n",
      "Batch No.: 53, Loss: 3.5891730785369873, Accuracy: 0.1708984375\n",
      "Batch No.: 54, Loss: 3.718987464904785, Accuracy: 0.155517578125\n",
      "Batch No.: 55, Loss: 3.6557695865631104, Accuracy: 0.15380859375\n",
      "Batch No.: 56, Loss: 3.6128854751586914, Accuracy: 0.161376953125\n",
      "Batch No.: 57, Loss: 3.711998701095581, Accuracy: 0.148681640625\n",
      "Batch No.: 58, Loss: 3.709406852722168, Accuracy: 0.1514892578125\n",
      "Batch No.: 59, Loss: 3.591505527496338, Accuracy: 0.1588134765625\n",
      "Batch No.: 60, Loss: 3.6163878440856934, Accuracy: 0.1514892578125\n",
      "Batch No.: 61, Loss: 3.6196749210357666, Accuracy: 0.16015625\n",
      "Batch No.: 62, Loss: 3.6365110874176025, Accuracy: 0.164306640625\n",
      "Batch No.: 63, Loss: 3.6939914226531982, Accuracy: 0.149169921875\n",
      "Batch No.: 64, Loss: 3.7310738563537598, Accuracy: 0.15625\n",
      "Batch No.: 66, Loss: 3.6519088745117188, Accuracy: 0.1597900390625\n",
      "Batch No.: 67, Loss: 3.7406578063964844, Accuracy: 0.153076171875\n",
      "Batch No.: 68, Loss: 3.5741782188415527, Accuracy: 0.174072265625\n",
      "Batch No.: 69, Loss: 3.5836739540100098, Accuracy: 0.17431640625\n",
      "Batch No.: 70, Loss: 3.658834457397461, Accuracy: 0.154052734375\n",
      "Batch No.: 71, Loss: 3.6243207454681396, Accuracy: 0.161376953125\n",
      "Batch No.: 72, Loss: 3.498382329940796, Accuracy: 0.1722412109375\n",
      "Batch No.: 73, Loss: 3.587108850479126, Accuracy: 0.1627197265625\n",
      "Batch No.: 74, Loss: 3.585278034210205, Accuracy: 0.1470947265625\n",
      "Batch No.: 76, Loss: 3.600186586380005, Accuracy: 0.1513671875\n",
      "Batch No.: 77, Loss: 3.584606170654297, Accuracy: 0.154296875\n",
      "Batch No.: 78, Loss: 3.4794921875, Accuracy: 0.1734619140625\n",
      "Batch No.: 79, Loss: 3.7869629859924316, Accuracy: 0.1297607421875\n",
      "Batch No.: 80, Loss: 3.6221413612365723, Accuracy: 0.1524658203125\n",
      "Batch No.: 81, Loss: 3.428161144256592, Accuracy: 0.17431640625\n",
      "Batch No.: 82, Loss: 3.637667655944824, Accuracy: 0.1448974609375\n",
      "Batch No.: 83, Loss: 3.588118076324463, Accuracy: 0.146484375\n",
      "Batch No.: 85, Loss: 3.569489002227783, Accuracy: 0.153564453125\n",
      "Batch No.: 86, Loss: 3.5960607528686523, Accuracy: 0.1507568359375\n",
      "Batch No.: 87, Loss: 3.4598701000213623, Accuracy: 0.1622314453125\n",
      "Batch No.: 88, Loss: 3.4703946113586426, Accuracy: 0.1611328125\n",
      "Batch No.: 89, Loss: 3.4774880409240723, Accuracy: 0.1553955078125\n",
      "Batch No.: 90, Loss: 3.439199924468994, Accuracy: 0.16015625\n",
      "Batch No.: 91, Loss: 3.446685314178467, Accuracy: 0.1522216796875\n",
      "Batch No.: 92, Loss: 3.4450483322143555, Accuracy: 0.157470703125\n",
      "Batch No.: 93, Loss: 3.4984130859375, Accuracy: 0.1463623046875\n",
      "Batch No.: 94, Loss: 3.4489357471466064, Accuracy: 0.1512451171875\n",
      "Batch No.: 95, Loss: 3.39579701423645, Accuracy: 0.1739501953125\n",
      "Batch No.: 96, Loss: 3.440460205078125, Accuracy: 0.163818359375\n",
      "Batch No.: 97, Loss: 3.405489921569824, Accuracy: 0.171630859375\n",
      "Batch No.: 98, Loss: 3.3848299980163574, Accuracy: 0.17041015625\n",
      "Batch No.: 99, Loss: 3.4107871055603027, Accuracy: 0.1595458984375\n",
      "Batch No.: 100, Loss: 3.4018900394439697, Accuracy: 0.158447265625\n",
      "Batch No.: 101, Loss: 3.277108907699585, Accuracy: 0.1761474609375\n",
      "Batch No.: 102, Loss: 3.4495673179626465, Accuracy: 0.1514892578125\n",
      "Batch No.: 103, Loss: 3.355111837387085, Accuracy: 0.158447265625\n",
      "Batch No.: 104, Loss: 3.3569414615631104, Accuracy: 0.170654296875\n",
      "Batch No.: 105, Loss: 3.371100425720215, Accuracy: 0.158935546875\n",
      "Batch No.: 106, Loss: 3.360464572906494, Accuracy: 0.1651611328125\n",
      "Batch No.: 107, Loss: 3.394253730773926, Accuracy: 0.1591796875\n",
      "Batch No.: 108, Loss: 3.445265293121338, Accuracy: 0.142333984375\n",
      "Batch No.: 109, Loss: 3.313570022583008, Accuracy: 0.156982421875\n",
      "Batch No.: 110, Loss: 3.33361554145813, Accuracy: 0.1627197265625\n",
      "Batch No.: 111, Loss: 3.438539505004883, Accuracy: 0.146240234375\n",
      "Batch No.: 112, Loss: 3.3377346992492676, Accuracy: 0.1585693359375\n",
      "Batch No.: 113, Loss: 3.358944892883301, Accuracy: 0.1571044921875\n",
      "Epoch 2/80\n"
     ]
    }
   ],
   "source": [
    "file = open(os.path.join(data_dir, data_file), mode = 'r')\n",
    "data = file.read()\n",
    "file.close()\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.358945</td>\n",
       "      <td>0.157104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.131432</td>\n",
       "      <td>0.164795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.792935</td>\n",
       "      <td>0.243774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.443726</td>\n",
       "      <td>0.345947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.115164</td>\n",
       "      <td>0.447144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1.891142</td>\n",
       "      <td>0.506714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.801800</td>\n",
       "      <td>0.530884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1.653776</td>\n",
       "      <td>0.556519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1.548871</td>\n",
       "      <td>0.572754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1.480200</td>\n",
       "      <td>0.587524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1.414975</td>\n",
       "      <td>0.600464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1.352152</td>\n",
       "      <td>0.616333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1.305672</td>\n",
       "      <td>0.623535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1.267200</td>\n",
       "      <td>0.631958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1.231936</td>\n",
       "      <td>0.641602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1.202520</td>\n",
       "      <td>0.644165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.159919</td>\n",
       "      <td>0.653931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1.137501</td>\n",
       "      <td>0.660156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1.106691</td>\n",
       "      <td>0.669312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1.079890</td>\n",
       "      <td>0.674561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1.065580</td>\n",
       "      <td>0.677246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1.029703</td>\n",
       "      <td>0.688477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1.030036</td>\n",
       "      <td>0.686401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.995224</td>\n",
       "      <td>0.695923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.983023</td>\n",
       "      <td>0.693115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.957263</td>\n",
       "      <td>0.700684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.941316</td>\n",
       "      <td>0.705078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.928291</td>\n",
       "      <td>0.710815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.900301</td>\n",
       "      <td>0.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.891049</td>\n",
       "      <td>0.722168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>0.671466</td>\n",
       "      <td>0.774658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>0.675685</td>\n",
       "      <td>0.770630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>0.686413</td>\n",
       "      <td>0.771973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>0.667845</td>\n",
       "      <td>0.787354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>55</td>\n",
       "      <td>0.643942</td>\n",
       "      <td>0.792236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>0.641515</td>\n",
       "      <td>0.790649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>0.632096</td>\n",
       "      <td>0.793579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>0.629128</td>\n",
       "      <td>0.791748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>0.617479</td>\n",
       "      <td>0.793457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>0.614687</td>\n",
       "      <td>0.795654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>0.623259</td>\n",
       "      <td>0.794067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>0.605066</td>\n",
       "      <td>0.803101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>0.606240</td>\n",
       "      <td>0.802124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>0.587201</td>\n",
       "      <td>0.810791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>0.584320</td>\n",
       "      <td>0.805542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>0.589127</td>\n",
       "      <td>0.804443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>0.583578</td>\n",
       "      <td>0.806641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>0.594631</td>\n",
       "      <td>0.801880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>0.579129</td>\n",
       "      <td>0.811401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>0.560432</td>\n",
       "      <td>0.816895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>0.566659</td>\n",
       "      <td>0.811523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>0.554629</td>\n",
       "      <td>0.818359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>0.551134</td>\n",
       "      <td>0.818237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>0.565444</td>\n",
       "      <td>0.811157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>0.575834</td>\n",
       "      <td>0.813110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>0.559111</td>\n",
       "      <td>0.820068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>0.551441</td>\n",
       "      <td>0.815430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>0.543258</td>\n",
       "      <td>0.823730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>0.559627</td>\n",
       "      <td>0.814209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>0.535046</td>\n",
       "      <td>0.822754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch      Loss  Accuracy\n",
       "0       1  3.358945  0.157104\n",
       "1       2  3.131432  0.164795\n",
       "2       3  2.792935  0.243774\n",
       "3       4  2.443726  0.345947\n",
       "4       5  2.115164  0.447144\n",
       "5       6  1.891142  0.506714\n",
       "6       7  1.801800  0.530884\n",
       "7       8  1.653776  0.556519\n",
       "8       9  1.548871  0.572754\n",
       "9      10  1.480200  0.587524\n",
       "10     11  1.414975  0.600464\n",
       "11     12  1.352152  0.616333\n",
       "12     13  1.305672  0.623535\n",
       "13     14  1.267200  0.631958\n",
       "14     15  1.231936  0.641602\n",
       "15     16  1.202520  0.644165\n",
       "16     17  1.159919  0.653931\n",
       "17     18  1.137501  0.660156\n",
       "18     19  1.106691  0.669312\n",
       "19     20  1.079890  0.674561\n",
       "20     21  1.065580  0.677246\n",
       "21     22  1.029703  0.688477\n",
       "22     23  1.030036  0.686401\n",
       "23     24  0.995224  0.695923\n",
       "24     25  0.983023  0.693115\n",
       "25     26  0.957263  0.700684\n",
       "26     27  0.941316  0.705078\n",
       "27     28  0.928291  0.710815\n",
       "28     29  0.900301  0.722900\n",
       "29     30  0.891049  0.722168\n",
       "..    ...       ...       ...\n",
       "50     51  0.671466  0.774658\n",
       "51     52  0.675685  0.770630\n",
       "52     53  0.686413  0.771973\n",
       "53     54  0.667845  0.787354\n",
       "54     55  0.643942  0.792236\n",
       "55     56  0.641515  0.790649\n",
       "56     57  0.632096  0.793579\n",
       "57     58  0.629128  0.791748\n",
       "58     59  0.617479  0.793457\n",
       "59     60  0.614687  0.795654\n",
       "60     61  0.623259  0.794067\n",
       "61     62  0.605066  0.803101\n",
       "62     63  0.606240  0.802124\n",
       "63     64  0.587201  0.810791\n",
       "64     65  0.584320  0.805542\n",
       "65     66  0.589127  0.804443\n",
       "66     67  0.583578  0.806641\n",
       "67     68  0.594631  0.801880\n",
       "68     69  0.579129  0.811401\n",
       "69     70  0.560432  0.816895\n",
       "70     71  0.566659  0.811523\n",
       "71     72  0.554629  0.818359\n",
       "72     73  0.551134  0.818237\n",
       "73     74  0.565444  0.811157\n",
       "74     75  0.575834  0.813110\n",
       "75     76  0.559111  0.820068\n",
       "76     77  0.551441  0.815430\n",
       "77     78  0.543258  0.823730\n",
       "78     79  0.559627  0.814209\n",
       "79     80  0.535046  0.822754\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = pd.read_csv(log_dir)\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
